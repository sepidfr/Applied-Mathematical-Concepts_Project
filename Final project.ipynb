{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification with Random Forest\n",
    "This report performs binary classification on the Breast Cancer Wisconsin dataset using a Random Forest classifier. We evaluate the model with metrics including accuracy, classification report, ROC-AUC, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Overview\n",
    "We load the dataset and preprocess it for binary classification. The diagnosis label is mapped to 1 (malignant) and 0 (benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset_Breast Cancer.csv')\n",
    "df.drop('id', axis=1, inplace=True)\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Splitting\n",
    "We normalize all features using MinMaxScaler and stratify the data split to preserve class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with GridSearchCV\n",
    "We use 10-fold cross-validation to find the best hyperparameters for the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended GridSearchCV parameter grid\n",
    "# Originally tested a broader range:\n",
    "# - n_estimators: [50, 100, 150, 200, 250]\n",
    "# - max_depth: [None, 5, 10, 15]\n",
    "# - min_samples_split: [2, 4, 6]\n",
    "# - min_samples_leaf: [1, 2, 3]\n",
    "#\n",
    "# Final performance was best around 100–200 estimators with moderate depth.\n",
    "# Wider values increased runtime without significant accuracy gain.\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "print(\"Best Parameters:\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Parameter Tuning\r\n",
    "\r\n",
    "A wide range of hyperparameters was tested using `GridSearchCV` to explore the effects of tree depth, ensemble size, and leaf conditions on model performance.\r\n",
    "\r\n",
    "**Parameters evaluated:**\r\n",
    "\r\n",
    "- `n_estimators`: [50, 100, 150, 200, 250]  \r\n",
    "- `max_depth`: [None, 5, 10, 15]  \r\n",
    "- `min_samples_split`: [2, 4, 6]  \r\n",
    "- `min_samples_leaf`: [1, 2, 3]\r\n",
    "\r\n",
    "The wider grid allowed observation of how the model behaves under different complexity levels.  \r\n",
    "Findings showed that:\r\n",
    "- Increasing `n_estimators` beyond 150 added training time with minimal performance gain.\r\n",
    "- Extremely deep trees (max_depth=15) showed signs of overfitting.\r\n",
    "- Balanced splits (split ≥ 2, leaf ≥ 1) provided the best generalization.\r\n",
    "\r\n",
    "The final grid was selected based on accuracy, training time, and model simplicity.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We calculate the accuracy and show the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report Analysis:**\n",
    "\n",
    "The classification report provides a detailed view of the model's predictive performance on each class.\n",
    "\n",
    "- **Precision** indicates how many of the positively predicted cases were truly positive (i.e., malignant).\n",
    "- **Recall (Sensitivity)** measures the model’s ability to identify all actual positives.\n",
    "- **F1-score** is the harmonic mean of precision and recall, useful for imbalanced datasets.\n",
    "\n",
    "High precision and recall for both classes demonstrate that the model is balanced and avoids bias toward either class. The F1-score being close to 1.0 indicates strong classification reliability. Misclassification is minimal, which is particularly critical in medical diagnostics to reduce false negatives (malignant predicted as benign)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "Visualizing the confusion matrix helps to understand how many cases were correctly or incorrectly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [0, 1]  # 0: Benign, 1: Malignant\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Malignant\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix Analysis:**\n",
    "\n",
    "The confusion matrix is a 2x2 table where:\n",
    "- The **diagonal values** represent correct predictions.\n",
    "- The **off-diagonal values** indicate misclassifications.\n",
    "\n",
    "- **True Positives (TP):** Correctly predicted malignant cases\n",
    "- **True Negatives (TN):** Correctly predicted benign cases\n",
    "- **False Positives (FP):** Benign predicted as malignant\n",
    "- **False Negatives (FN):** Malignant predicted as benign\n",
    "\n",
    "In a medical context, **false negatives are particularly critical** because missing a malignant case can delay treatment. The confusion matrix here shows low FN and FP rates, affirming the model's suitability for early detection and diagnosis support.\n",
    "###  Confusion Matrix Analysis\r\n",
    "\r\n",
    "The confusion matrix summarizes the classifier's performance on the test set, with true class labels on the vertical axis and predicted labels on the horizontal axis.\r\n",
    "\r\n",
    "|                        | Predicted: Benign | Predicted: Malignant |\r\n",
    "|------------------------|------------------|----------------------|\r\n",
    "| **Actual: Benign**     | 72               | 0                    |\r\n",
    "| **Actual: Malignant**  | 3                | 39                   |\r\n",
    "\r\n",
    "#### Key Observations:\r\n",
    "- **True Negatives (TN)** = 72 → Benign correctly predicted as benign\r\n",
    "- **True Positives (TP)** = 39 → Malignant correctly predicted as malignant\r\n",
    "- **False Negatives (FN)** = 3 → Malignant misclassified as benign\r\n",
    "- **False Positives (FP)** = 0 → No benign cases misclassified as malignant\r\n",
    "\r\n",
    "#### Interpretation:\r\n",
    "- The model achieves **perfect precision for malignant predictions** (i.e., no false alarms).\r\n",
    "- However, it missed **3 malignant cases**, indicating a **false negative rate of ~7.1%**.\r\n",
    "- This means the model failed to detect ~7% of actual cancers, which in a medical context, is a critical consideration.\r\n",
    "\r\n",
    "#### Clinical Implications:\r\n",
    "- **False negatives are more dangerous** than false positives in cancer detection since undetected malignant cases can delay diagnosis and treatment.\r\n",
    "- While the overall accuracy remains high, further calibration (e.g., adjusting the classification threshold) could help reduce FN without significantly increasing FP.\r\n",
    "\r\n",
    "#### Metric Summary:\r\n",
    "- **Sensitivity (Recall)** = 39 / (39 + 3) ≈ 0.9286\r\n",
    "- **Specificity** = 72 / (72 + 0) = 1.0\r\n",
    "- **Precision** = 39 / (39 + 0) = 1.0\r\n",
    "\r\n",
    "This matrix shows a high-performing model with some potential for improving sensitivity.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve and AUC Score\n",
    "The ROC curve evaluates classifier performance across all thresholds. AUC summarizes this performance as a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC Curve Analysis:**\n",
    "\n",
    "The ROC curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at different threshold settings.\n",
    "\n",
    "The closer the ROC curve is to the top-left corner, the better the classifier performs. A diagonal line from (0, 0) to (1, 1) represents random guessing.\n",
    "\n",
    "The **Area Under the Curve (AUC)** is a scalar measure summarizing the ROC curve. AUC = 1.0 is perfect classification; AUC = 0.5 is random guessing.\n",
    "\n",
    "In this analysis, the ROC curve arcs significantly toward the top-left, and the AUC value (e.g., 0.98+) indicates the model is highly effective at separating benign and malignant cases under varying classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "We visualize the most influential features in the prediction decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1][:10]\n",
    "feature_names = X.columns[indices]\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances[indices], y=feature_names)\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance Analysis:**\n",
    "\n",
    "This bar chart visualizes the relative importance of each input feature based on how much it reduces impurity (Gini index) across the trees in the ensemble.\n",
    "\n",
    "Features such as:\n",
    "- **mean radius**\n",
    "- **worst perimeter**\n",
    "- **mean concavity**\n",
    "\n",
    "typically dominate the decision space, indicating they carry the strongest signal for distinguishing malignancy.\n",
    "\n",
    "Clinically, this aligns with domain knowledge: irregular size and contour are strong indicators of malignant pathology. By identifying the top contributing features, the model provides interpretability and transparency, which is valuable in sensitive applications such as medical diagnostics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
